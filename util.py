import aiohttp
import random
from datetime import datetime
from typing import Optional, Dict, Any, List
from pathlib import Path
import json
import asyncio
import re
import time
import subprocess
import shlex
import base64
from urllib.parse import urlparse, parse_qs

from cookie import CookieManager
from config import WEBHOOK_URL, PROXY_URL, USE_PROXY
from emoji import get_emoji_and_type

# User-Agentæ± 
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:120.0) Gecko/20100101 Firefox/120.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0"
]

# æ–‡ä»¶è·¯å¾„ç›¸å…³é…ç½®
DATA_DIR = Path("data")
DATA_DIR.mkdir(exist_ok=True)

# æ–‡ä»¶åé…ç½®
LISTING_RAW_FILE = "listing_raw.html"
LISTING_PARSED_FILE = "listing_parsed.json"

# æ·»åŠ é”™è¯¯æ¨é€é™åˆ¶ç›¸å…³çš„å…¨å±€å˜é‡
ERROR_MSG_LIMIT = 5  # æ¯ä¸ªæ—¶é—´çª—å£å†…çš„æœ€å¤§é”™è¯¯æ¨é€æ¬¡æ•°
ERROR_MSG_WINDOW = 3600  # æ—¶é—´çª—å£å¤§å°(ç§’)
error_msg_count = 0
last_error_reset_time = datetime.now()

# åˆå§‹åŒ–CookieManager
cookie_manager = CookieManager()

def build_article_link(title: str, code: str) -> str:
    """æ„å»ºæ–‡ç« é“¾æ¥
    
    Args:
        title: æ–‡ç« æ ‡é¢˜
        code: æ–‡ç« code(ä¸æ˜¯id)
        
    Returns:
        æ ¼å¼åŒ–åçš„æ–‡ç« é“¾æ¥
    """
    base_url = "https://www.binance.com/en/support/announcement/"
    
    # å¤„ç†æ ‡é¢˜æ ¼å¼
    formatted_title = title.lower()
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ç§»é™¤ç‰¹å®šæ ‡ç‚¹ç¬¦å·ï¼Œå°†æ’‡å·æ›¿æ¢ä¸ºè¿å­—ç¬¦
    formatted_title = re.sub(r'[()!?.,:""#&]', '', formatted_title)
    formatted_title = formatted_title.replace("'", "-")
    # å°†è¿ç»­çš„ç©ºæ ¼æ›¿æ¢ä¸ºå•ä¸ªç ´æŠ˜å·
    formatted_title = re.sub(r'\s+', '-', formatted_title)
    
    return f"{base_url}{formatted_title}-{code}"

def build_message(title: str, 
                 release_date: str, 
                 link: str
                 ) -> str:
    """æ„å»ºé€šç”¨çš„æ¨é€æ¶ˆæ¯
    
    Args:
        title: å…¬å‘Šæ ‡é¢˜
        release_date: å‘å¸ƒæ—¶é—´
        link: æ–‡ç« é“¾æ¥
        
    Returns:
        æ ¼å¼åŒ–çš„æ¶ˆæ¯å­—ç¬¦ä¸²
    """
    emoji, type = get_emoji_and_type(title)
    
    return (
        f"{emoji} {type}\n"
        f"ğŸ“Œ: {title}\n"
        f"ğŸ•’: {release_date}\n"
        f"ğŸ”—: {link if link else 'æ— é“¾æ¥'}"
    )

async def send_message_async(message_content: str, is_error: bool = False) -> None:
    """å‘é€æ¶ˆæ¯åˆ°ä¼ä¸šå¾®ä¿¡æœºå™¨äºº
    
    Args:
        message_content: è¦å‘é€çš„æ¶ˆæ¯å†…å®¹
        is_error: æ˜¯å¦ä¸ºé”™è¯¯æ¶ˆæ¯
    """
    global error_msg_count, last_error_reset_time
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡ç½®é”™è¯¯è®¡æ•°
    now = datetime.now()
    if (now - last_error_reset_time).total_seconds() >= ERROR_MSG_WINDOW:
        error_msg_count = 0
        last_error_reset_time = now
    
    # å¦‚æœæ˜¯é”™è¯¯æ¶ˆæ¯ä¸”å·²è¾¾åˆ°é™åˆ¶,åˆ™åªè®°å½•æ—¥å¿—
    if is_error:
        if error_msg_count >= ERROR_MSG_LIMIT:
            log_with_time(f"Error message suppressed (limit reached): {message_content}")
            return
        error_msg_count += 1
        
    headers = {'Content-Type': 'application/json'}
    payload = {
        "msgtype": "text",
        "text": {
            "content": message_content
        }
    }
    
    log_with_time(f"Sending message: {message_content}")

    proxy = PROXY_URL if USE_PROXY else None
    async with aiohttp.ClientSession() as session:
        async with session.post(WEBHOOK_URL, json=payload, headers=headers, proxy=proxy) as response:
            if response.status == 200:
                log_with_time("Message sent successfully!")
            else:
                log_with_time(f"Failed to send message: {response.status}")

def log_with_time(message: str, module: str = '') -> None:
    """æ‰“å°å¸¦æ—¶é—´æˆ³å’Œæ¨¡å—åçš„æ¶ˆæ¯"""
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{current_time}] {message}")

def get_random_user_agent() -> str:
    """éšæœºé€‰æ‹©ä¸€ä¸ªç”¨æˆ·ä»£ç†"""
    return random.choice(USER_AGENTS)

async def get_headers(referer: str = '') -> Dict[str, str]:
    """ç”Ÿæˆé«˜åº¦æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨çš„è¯·æ±‚å¤´"""
    cookie = cookie_manager.get_cookies()
    if not cookie:
        try:
            cookie = await cookie_manager.update_cookies()
        except Exception as e:
            log_with_time(f"Failed to get cookies: {e}")
            raise
    
    # éšæœºé€‰æ‹©ç”¨æˆ·ä»£ç†
    user_agent = get_random_user_agent()
    
    # ç”Ÿæˆéšæœºçš„æµè§ˆå™¨ç‰¹å¾
    chrome_version = random.randint(100, 135)
    platform = random.choice(["macOS", "Windows"])
    
    # æ·»åŠ æ›´å¤šçœŸå®çš„æµè§ˆå™¨æŒ‡çº¹
    headers = {
        'authority': 'www.binance.com',
        'method': 'GET',
        'scheme': 'https',
        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'accept-encoding': 'gzip, deflate, br, zstd',
        'accept-language': 'zh-CN,zh;q=0.9,en;q=0.8',
        'cache-control': 'max-age=0',
        'cookie': cookie,
        'sec-ch-ua': f'"Google Chrome";v="{chrome_version}", "Not-A.Brand";v="8", "Chromium";v="{chrome_version}"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': f'"{platform}"',
        'sec-fetch-dest': 'document',
        'sec-fetch-mode': 'navigate',
        'sec-fetch-site': 'same-origin',
        'sec-fetch-user': '?1',
        'upgrade-insecure-requests': '1',
        'user-agent': user_agent,
        # æ·»åŠ éšæœºæµè§ˆå™¨æŒ‡çº¹å‚æ•°
        'viewport-width': f'{random.choice([1280, 1440, 1920])}',
        'device-memory': f'{random.choice([4, 8, 16])}',
        'dpr': f'{random.choice([1, 2])}',
        'priority': 'u=0, i'
    }
    
    # å¦‚æœæä¾›äº†refererï¼Œæ·»åŠ åˆ°å¤´éƒ¨
    if referer:
        headers['referer'] = referer
    else:
        # ä½¿ç”¨éšæœºçš„å‰å¯¼é¡µé¢ä½œä¸ºreferer
        referers = [
            'https://www.binance.com/en',
            'https://www.binance.com/en/markets',
            'https://www.binance.com/en/trade',
            'https://www.binance.com/en/my/dashboard'
        ]
        headers['referer'] = random.choice(referers)
        
    return headers

async def fetch_with_curl(url: str, cookie: str, proxy: str = None) -> Optional[str]:
    """ä½¿ç”¨curlå‘½ä»¤è·å–ç½‘é¡µå†…å®¹"""
    try:
        cmd = [
            'curl', 
            url,
            '-H', f'Cookie: {cookie}',
            '-H', 'User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36',
            '-H', 'Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            '-H', 'Accept-Language: zh-CN,zh;q=0.9,en;q=0.8,zh-TW;q=0.7',
            '-H', 'Cache-Control: max-age=0',
            '-H', 'sec-ch-ua: "Google Chrome";v="135", "Not-A.Brand";v="8", "Chromium";v="135"',
            '-H', 'sec-ch-ua-mobile: ?0',
            '-H', 'sec-ch-ua-platform: "macOS"',
            '-H', 'sec-fetch-dest: document',
            '-H', 'sec-fetch-mode: navigate',
            '-H', 'sec-fetch-site: same-origin',
            '-H', 'sec-fetch-user: ?1',
            '-H', 'upgrade-insecure-requests: 1',
            '--compressed',
            '-s',  # é™é»˜æ¨¡å¼
            '-L'   # è·Ÿéšé‡å®šå‘
        ]
        
        # å¦‚æœæœ‰ä»£ç†ï¼Œæ·»åŠ ä»£ç†è®¾ç½®
        if proxy:
            cmd.extend(['--proxy', proxy])
            
        log_with_time(f"Executing curl command for {url}")
        
        # ä½¿ç”¨å¼‚æ­¥å­è¿›ç¨‹è¿è¡Œcurl
        process = await asyncio.create_subprocess_exec(
            *cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )
        
        stdout, stderr = await process.communicate()
        
        if process.returncode != 0:
            log_with_time(f"Curl command failed: {stderr.decode()}")
            return None
            
        return stdout.decode('utf-8')
    except Exception as e:
        log_with_time(f"Error executing curl: {e}")
        return None

async def handle_human_verification(content: str) -> bool:
    """å¤„ç†äººæœºéªŒè¯é¡µé¢ï¼Œæé†’ç”¨æˆ·éœ€è¦æ‰‹åŠ¨éªŒè¯"""
    # æ£€æŸ¥æ˜¯å¦åŒ…å«äººæœºéªŒè¯çš„ç‰¹å¾
    if "Human Verification" in content or "captcha" in content or "AwsWafIntegration" in content:
        log_with_time("âš ï¸ æ£€æµ‹åˆ°äººæœºéªŒè¯é¡µé¢ï¼")
        
        # ä»HTMLå†…å®¹ä¸­æå–AWS WAFç›¸å…³ä¿¡æ¯
        aws_token_match = re.search(r'aws-waf-token=([^;"]+)', content)
        aws_token = aws_token_match.group(1) if aws_token_match else None
        
        # å°è¯•è‡ªåŠ¨æ›´æ–°cookie
        try:
            new_cookie = await cookie_manager.update_cookies()
            if new_cookie and new_cookie != cookie_manager.get_cookies():
                log_with_time("âœ… æˆåŠŸè‡ªåŠ¨æ›´æ–°Cookie")
                return False  # ä¸å‘é€é€šçŸ¥ï¼Œç›´æ¥é‡è¯•
        except Exception as e:
            log_with_time(f"âŒ è‡ªåŠ¨æ›´æ–°Cookieå¤±è´¥: {e}")
        
        # å¦‚æœè‡ªåŠ¨æ›´æ–°å¤±è´¥ï¼Œå‘é€é€šçŸ¥ç»™ç”¨æˆ·
        message = (
            "âš ï¸ äººæœºéªŒè¯æ‹¦æˆª\n"
            "å¸å®‰ç½‘ç«™å·²å¯ç”¨äººæœºéªŒè¯ï¼Œéœ€è¦æ‚¨æ‰‹åŠ¨æ“ä½œï¼š\n"
            "1. è¯·è®¿é—®å¸å®‰å…¬å‘Šé¡µé¢å¹¶å®ŒæˆéªŒè¯\n"
            "2. å®ŒæˆéªŒè¯åå¤åˆ¶æ–°cookieå¹¶æ›´æ–°é…ç½®\n"
            f"AWS Token: {aws_token if aws_token else 'æœªæ‰¾åˆ°'}"
        )
        await send_message_async(message, is_error=True)
        return True
    
    return False

async def fetch_and_save_html_content(url: str, filename: str, max_retries: int = 3) -> Optional[str]:
    """è·å–å¹¶ä¿å­˜HTMLå†…å®¹,æ”¯æŒcookieè‡ªåŠ¨æ›´æ–°"""
    for attempt in range(max_retries):
        try:
            # ä½¿ç”¨å›ºå®šçš„æ—¶é—´æˆ³å‚æ•°
            if '?' in url:
                base_url, params = url.split('?', 1)
                params_dict = {}
                for param in params.split('&'):
                    if '=' in param:
                        k, v = param.split('=', 1)
                        params_dict[k] = v
                
                # ç¡®ä¿æœ‰navIdå‚æ•°
                if 'navId' not in params_dict:
                    params_dict['navId'] = '48'
                    
                # ä½¿ç”¨å›ºå®šæ—¶é—´æˆ³
                params_dict['t'] = str(int(time.time()))
                
                # é‡å»ºURL
                query_params = '&'.join([f"{k}={v}" for k, v in params_dict.items()])
                url_with_params = f"{base_url}?{query_params}"
            else:
                # å¦‚æœURLæ²¡æœ‰å‚æ•°ï¼Œæ·»åŠ å¿…è¦çš„å‚æ•°
                url_with_params = f"{url}?navId=48&t={int(time.time())}"
            
            log_with_time(f"Starting request to {url_with_params}")
            
            # è·å–cookie
            cookie = cookie_manager.get_cookies()
            if not cookie:
                cookie = await cookie_manager.update_cookies()
                
            log_with_time(f"Using cookie: {cookie[:50]}...")
            
            proxy = PROXY_URL if USE_PROXY else None
            log_with_time(f"Attempt {attempt + 1}/{max_retries} with proxy: {proxy}")
            
            # é¦–å…ˆå°è¯•ä½¿ç”¨curlï¼Œå®ƒé€šå¸¸æ¯”aiohttpæ›´å¥½åœ°æ¨¡æ‹Ÿæµè§ˆå™¨
            curl_content = await fetch_with_curl(url_with_params, cookie, proxy)
            if curl_content:
                log_with_time("ğŸ”„ Successfully fetched content with curl")
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯äººæœºéªŒè¯é¡µé¢
                if await handle_human_verification(curl_content):
                    # å¦‚æœæ˜¯äººæœºéªŒè¯é¡µé¢ï¼Œä¿å­˜åˆ°æ–‡ä»¶ä»¥ä¾¿åˆ†æ
                    file_path = DATA_DIR / f"captcha_{filename}"
                    file_path.write_text(curl_content, encoding='utf-8')
                    log_with_time(f"ğŸ’¾ Captcha page saved to {file_path}")
                    return None
                
                # ä¿å­˜å†…å®¹åˆ°æ–‡ä»¶
                file_path = DATA_DIR / filename
                file_path.write_text(curl_content, encoding='utf-8')
                log_with_time(f"ğŸ’¾ Content saved to {file_path}")
                return curl_content
            else:
                log_with_time("âŒ Failed to fetch content with curl, trying aiohttp...")
            
            # å¦‚æœcurlå¤±è´¥ï¼Œå°è¯•ä½¿ç”¨aiohttp
            try:
                headers = await get_headers()
                regular_headers = {k: v for k, v in headers.items() if not k.startswith(':')}
                
                # æ·»åŠ éšæœºçš„å¯æ¥å—æ ¼å¼å’Œç¼–ç ï¼Œä»¥åŠæ›´å¤šçš„æµè§ˆå™¨ç‰¹å¾
                regular_headers['Accept-Encoding'] = 'gzip, deflate, br'
                regular_headers['Connection'] = 'keep-alive'
                
                # æ·»åŠ refererå¤´ï¼Œæ¨¡æ‹Ÿä»å¸å®‰ä¸»é¡µè®¿é—®
                regular_headers['Referer'] = 'https://www.binance.com/en'
                
                async with aiohttp.ClientSession() as session:
                    async with session.get(
                        url_with_params, 
                        headers=regular_headers, 
                        proxy=proxy,
                        timeout=aiohttp.ClientTimeout(total=30)
                    ) as response:
                        log_with_time(f"ğŸ”„ Response status: {response.status}")
                        
                        if response.status == 200:
                            content = await response.text()
                            
                            # æ£€æŸ¥æ˜¯å¦æ˜¯äººæœºéªŒè¯é¡µé¢
                            if await handle_human_verification(content):
                                # å¦‚æœæ˜¯äººæœºéªŒè¯é¡µé¢ï¼Œä¿å­˜åˆ°æ–‡ä»¶ä»¥ä¾¿åˆ†æ
                                file_path = DATA_DIR / f"captcha_{filename}"
                                file_path.write_text(content, encoding='utf-8')
                                log_with_time(f"ğŸ’¾ Captcha page saved to {file_path}")
                                return None
                            
                            # ä¿å­˜å†…å®¹åˆ°æ–‡ä»¶
                            file_path = DATA_DIR / filename
                            file_path.write_text(content, encoding='utf-8')
                            log_with_time(f"ğŸ’¾ Content saved to {file_path}")
                            return content
                        elif response.status == 202:
                            log_with_time("ğŸ”‘ Cookie expired, updating...")
                            await cookie_manager.update_cookies()
                            continue
                        else:
                            log_with_time(f"âŒ Request failed with status: {response.status}")
            except aiohttp.ClientError as e:
                log_with_time(f"aiohttp request failed: {e}")
                    
        except Exception as e:
            log_with_time(f"Error in attempt {attempt + 1}: {e}")
            if attempt == max_retries - 1:
                raise
            
        # æŒ‡æ•°é€€é¿
        retry_delay = 2 ** attempt
        log_with_time(f"Retrying in {retry_delay} seconds...")
        await asyncio.sleep(retry_delay)
    
    # æ‰€æœ‰å°è¯•éƒ½å¤±è´¥åï¼Œå‘é€é€šçŸ¥
    await send_message_async("âŒ æ— æ³•è·å–å¸å®‰å…¬å‘Šåˆ—è¡¨ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–æ›´æ–°Cookie", is_error=True)
    return None

async def update_cookie_from_file(file_path: str) -> bool:
    """ä»æ–‡ä»¶æ›´æ–°cookie"""
    try:
        path = Path(file_path)
        if not path.exists():
            log_with_time(f"Cookieæ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return False
            
        cookie = path.read_text().strip()
        if not cookie:
            log_with_time("Cookieæ–‡ä»¶ä¸ºç©º")
            return False
            
        # æ›´æ–°cookie
        cookie_manager.update_cookie_from_str(cookie)
        log_with_time("æˆåŠŸä»æ–‡ä»¶æ›´æ–°cookie")
        return True
    except Exception as e:
        log_with_time(f"ä»æ–‡ä»¶æ›´æ–°cookieå¤±è´¥: {e}")
        return False

def save_html_content(response_text: str, filename: str) -> None:
    """ä¿å­˜HTMLå†…å®¹åˆ°dataç›®å½•ä¸‹çš„æ–‡ä»¶
    
    Args:
        response_text: è¦ä¿å­˜çš„HTMLå†…å®¹
        filename: æ–‡ä»¶å
    """
    try:
        file_path = DATA_DIR / filename
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(response_text)
        log_with_time(f"Raw HTML saved to {file_path}")
    except Exception as e:
        log_with_time(f"Error saving HTML content: {e}")

def get_last_articles_from_file(filename: str = LISTING_PARSED_FILE) -> set:
    """ä»æœ¬åœ°JSONæ–‡ä»¶ä¸­è¯»å–ä¸Šæ¬¡çš„æ–‡ç« IDé›†åˆ
    
    Args:
        filename: JSONæ–‡ä»¶åï¼Œé»˜è®¤ä¸º"listing_parsed.json"
        
    Returns:
        æ–‡ç« IDé›†åˆ
    """
    try:
        json_path = DATA_DIR / filename
        if not json_path.exists():
            log_with_time(f"No existing file found at {json_path}")
            return set()
            
        with open(json_path, 'r', encoding='utf-8') as f:
            json_data = json.load(f)
            
        # æŸ¥æ‰¾åŒ…å« catalogDetail çš„è·¯ç”±
        for route_content in json_data['appState']['loader']['dataByRouteId'].values():
            if 'catalogDetail' in route_content:
                articles = route_content['catalogDetail']['articles']
                log_with_time(f"Loaded {len(articles)} articles from {filename}")
                return {article['id'] for article in articles}
                
        log_with_time(f"No articles found in {filename}")
        return set()
    except Exception as e:
        log_with_time(f"Error reading last articles from file: {e}")
        return set()