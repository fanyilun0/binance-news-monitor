import requests
import aiohttp
import random
from datetime import datetime
from typing import Optional, Dict, Any, List
import os
from pathlib import Path
import json
import asyncio
import re

from config import WEBHOOK_URL, PROXY_URL, USE_PROXY, COOKIE_FILE

# User-Agentæ± 
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:120.0) Gecko/20100101 Firefox/120.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0"
]

# Emojiæ˜ å°„å­—å…¸
EMOJI_MAPPINGS = {
    # ä¸Šå¸å…¬å‘Š
    'listing': {
        "Introducing": "ğŸš€",
        "ä¸Šçº¿": "ğŸš€",
        "Launchpool": "ğŸŒ±", 
        "Futures": "ğŸ“ˆ",
        "Options": "ğŸ“Š",
        "Margin": "ğŸ’¹"
    },
    # æ–°é—»å…¬å‘Š
    'news': {
        "Binance": "ğŸ“¢",
        "å¸å®‰": "ğŸ“¢",
        "Announcement": "ğŸ“£",
        "å…¬å‘Š": "ğŸ“£",
        "Notice": "â„¹ï¸",
        "é€šçŸ¥": "â„¹ï¸"
    },
    # æ´»åŠ¨å…¬å‘Š
    'activities': {
        "Rewards": "ğŸ",
        "å¥–åŠ±": "ğŸ",
        "Campaign": "ğŸ¯",
        "æ´»åŠ¨": "ğŸ¯",
        "Airdrop": "ğŸª‚",
        "ç©ºæŠ•": "ğŸª‚",
        "Staking": "ğŸ†",
        "è´¨æŠ¼": "ğŸ†"
    }
}

# æ–‡ä»¶è·¯å¾„ç›¸å…³é…ç½®
DATA_DIR = Path("data")
DATA_DIR.mkdir(exist_ok=True)

# æ–‡ä»¶åé…ç½®
LISTING_RAW_FILE = "listing_raw.html"
LISTING_PARSED_FILE = "listing_parsed.json"

# æ·»åŠ é”™è¯¯æ¨é€é™åˆ¶ç›¸å…³çš„å…¨å±€å˜é‡
ERROR_MSG_LIMIT = 5  # æ¯ä¸ªæ—¶é—´çª—å£å†…çš„æœ€å¤§é”™è¯¯æ¨é€æ¬¡æ•°
ERROR_MSG_WINDOW = 3600  # æ—¶é—´çª—å£å¤§å°(ç§’)
error_msg_count = 0
last_error_reset_time = datetime.now()

def get_emoji_for_type(title: str, announcement_type: str) -> str:
    """æ ¹æ®å…¬å‘Šæ ‡é¢˜å’Œç±»å‹è¿”å›ç›¸åº”çš„emoji
    
    Args:
        title: å…¬å‘Šæ ‡é¢˜
        announcement_type: å…¬å‘Šç±»å‹('listing', 'news', 'activities')
        
    Returns:
        å¯¹åº”çš„emojiå­—ç¬¦ä¸²
    """
    emoji_map = EMOJI_MAPPINGS.get(announcement_type, {})
    return next((emoji for keyword, emoji in emoji_map.items() 
                if keyword in title), "â„¹ï¸")

def build_article_link(title: str, code: str) -> str:
    """æ„å»ºæ–‡ç« é“¾æ¥
    
    Args:
        title: æ–‡ç« æ ‡é¢˜
        code: æ–‡ç« code(ä¸æ˜¯id)
        
    Returns:
        æ ¼å¼åŒ–åçš„æ–‡ç« é“¾æ¥
    """
    base_url = "https://www.binance.com/en/support/announcement/"
    
    # å¤„ç†æ ‡é¢˜æ ¼å¼
    formatted_title = title.lower()
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ç§»é™¤ç‰¹å®šæ ‡ç‚¹ç¬¦å·ï¼Œå°†æ’‡å·æ›¿æ¢ä¸ºè¿å­—ç¬¦
    formatted_title = re.sub(r'[()!?.,:â€œâ€#&]', '', formatted_title)
    formatted_title = formatted_title.replace("'", "-")
    # å°†è¿ç»­çš„ç©ºæ ¼æ›¿æ¢ä¸ºå•ä¸ªç ´æŠ˜å·
    formatted_title = re.sub(r'\s+', '-', formatted_title)
    
    return f"{base_url}{formatted_title}-{code}"

def build_message(title: str, 
                 release_date: str, 
                 link: str,
                 announcement_type: str,
                 is_initial: bool = False) -> str:
    """æ„å»ºé€šç”¨çš„æ¨é€æ¶ˆæ¯
    
    Args:
        title: å…¬å‘Šæ ‡é¢˜
        release_date: å‘å¸ƒæ—¶é—´
        link: æ–‡ç« é“¾æ¥
        announcement_type: å…¬å‘Šç±»å‹('listing', 'news', 'activities')
        is_initial: æ˜¯å¦ä¸ºåˆå§‹åŒ–æ¶ˆæ¯
        
    Returns:
        æ ¼å¼åŒ–çš„æ¶ˆæ¯å­—ç¬¦ä¸²
    """
    emoji = get_emoji_for_type(title, announcement_type)
    
    # æ ¹æ®ä¸åŒç±»å‹è®¾ç½®ä¸åŒçš„å‰ç¼€
    prefix_map = {
        'listing': "æ–°å¸ç§ä¸Šçº¿å…¬å‘Š",
        'news': "å¸å®‰æ–°é—»å…¬å‘Š",
        'activities': "å¸å®‰æ´»åŠ¨å…¬å‘Š"
    }
    
    if is_initial:
        prefix = f"ğŸ“¢ Initial {prefix_map.get(announcement_type, '')} Alert ğŸ“¢"
    else:
        prefix = f"{emoji} {prefix_map.get(announcement_type, '')} ğŸ“¢"
    
    return (
        f"{prefix}\n"
        f"æ ‡é¢˜: {title}\n"
        f"æ—¶é—´: {release_date}\n"
        f"é“¾ï¿½ï¿½ï¿½: {link if link else 'æ— é“¾æ¥'}"
    )

async def send_message_async(message_content: str, is_error: bool = False) -> None:
    """å‘é€æ¶ˆæ¯åˆ°ä¼ä¸šå¾®ä¿¡æœºå™¨äºº
    
    Args:
        message_content: è¦å‘é€çš„æ¶ˆæ¯å†…å®¹
        is_error: æ˜¯å¦ä¸ºé”™è¯¯æ¶ˆæ¯
    """
    global error_msg_count, last_error_reset_time
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡ç½®é”™è¯¯è®¡æ•°
    now = datetime.now()
    if (now - last_error_reset_time).total_seconds() >= ERROR_MSG_WINDOW:
        error_msg_count = 0
        last_error_reset_time = now
    
    # å¦‚æœæ˜¯é”™è¯¯æ¶ˆæ¯ä¸”å·²è¾¾åˆ°é™åˆ¶,åˆ™åªè®°å½•æ—¥å¿—
    if is_error:
        if error_msg_count >= ERROR_MSG_LIMIT:
            log_with_time(f"Error message suppressed (limit reached): {message_content}")
            return
        error_msg_count += 1
        
    headers = {'Content-Type': 'application/json'}
    payload = {
        "msgtype": "text",
        "text": {
            "content": message_content
        }
    }
    
    log_with_time(f"Sending message: {message_content}")

    proxy = PROXY_URL if USE_PROXY else None
    async with aiohttp.ClientSession() as session:
        async with session.post(WEBHOOK_URL, json=payload, headers=headers, proxy=proxy) as response:
            if response.status == 200:
                log_with_time("Message sent successfully!")
            else:
                log_with_time(f"Failed to send message: {response.status}")

def log_with_time(message: str, module: str = '') -> None:
    """æ‰“å°å¸¦æ—¶é—´æˆ³å’Œæ¨¡å—åçš„æ¶ˆæ¯
    
    Args:
        message: æ—¥å¿—æ¶ˆæ¯
        module: æ¨¡å—åç§°
    """
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{current_time}] {message}")

def get_random_headers(referer: str = '') -> Dict[str, str]:
    """ç”Ÿæˆéšæœºè¯·æ±‚å¤´"""
    return {
        'authority': 'www.binance.com',
        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'accept-encoding': 'gzip, deflate, br, zstd',
        'accept-language': 'zh-CN,zh;q=0.9,en;q=0.8,zh-TW;q=0.7',
        'cache-control': 'max-age=0',
        'User-Agent': random.choice(USER_AGENTS),
        'cookie': get_random_cookie(),
        'sec-ch-ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"Windows"',
        'sec-fetch-dest': 'document',
        'sec-fetch-mode': 'navigate',
        'sec-fetch-site': 'same-origin',
        'sec-fetch-user': '?1',
        'upgrade-insecure-requests': '1'
    }

async def initialize_monitor(save_and_parse_func: callable, 
                           send_notifications_func: callable,
                           always_notify: bool = False) -> tuple:
    """é€šç”¨çš„ç›‘æ§åˆå§‹åŒ–å‡½æ•°"""
    initial_articles = save_and_parse_func()
    if initial_articles:
        article_ids = {article['id'] for article in initial_articles}
        log_with_time(f"Initialized with {len(article_ids)} articles")
        
        if always_notify:
            await send_notifications_func(initial_articles, is_initial=True)
            
        return article_ids, initial_articles
    return set(), []

def load_cookies() -> List[str]:
    """ä»æ–‡ä»¶åŠ è½½cookieåˆ—è¡¨"""
    cookie_path = Path(COOKIE_FILE)
    if not cookie_path.exists():
        log_with_time(f"No existing file found at {cookie_path}")
        return []

    with open(cookie_path, 'r', encoding='utf-8') as f:
        cookies = [line.strip() for line in f if line.strip()]
    
    return cookies

def get_random_cookie() -> str:
    """è·å–éšæœºcookie"""
    cookies = load_cookies()
    return random.choice(cookies)

async def fetch_and_save_html_content(url: str, filename: str, max_retries: int = 3) -> Optional[str]:
    """è·å–å¹¶ä¿å­˜HTMLå†…å®¹,ä½¿ç”¨éšæœºcookie"""
    headers = get_random_headers()
    log_with_time(f"Starting request to {url}")
    log_with_time(f"Using cookie: {headers['cookie'][:50]}...")
    
    for attempt in range(max_retries):
        try:
            proxy = PROXY_URL if USE_PROXY else None
            log_with_time(f"Attempt {attempt + 1}/{max_retries} with proxy: {proxy}")
            
            async with aiohttp.ClientSession() as session:
                async with session.get(url, headers=headers, proxy=proxy) as response:
                    log_with_time(f"Response status: {response.status}")
                    
                    if response.status == 202:
                        log_with_time("Received 202 status, retrying...")
                        await asyncio.sleep(2)
                        continue
                        
                    if response.status != 200:
                        error_msg = f"Request failed with status {response.status} for URL: {url}"
                        log_with_time(error_msg)
                        if attempt == max_retries - 1:  # æœ€åä¸€æ¬¡é‡è¯•
                            error_msg = f"All {max_retries} attempts failed for {url}"
                            log_with_time(error_msg)
                            await send_message_async(error_msg, is_error=True)
                        continue
                        
                    response_text = await response.text()
                    
                    # éªŒè¯å“åº”å†…å®¹æ˜¯å¦æœ‰æ•ˆ
                    is_valid = "binance" in response_text.lower()
                    log_with_time(f"Response validation: {'Valid' if is_valid else 'Invalid'} (contains 'binance': {is_valid})")
                    
                    if not is_valid:
                        log_with_time(f"Invalid response with cookie: {headers['cookie'][:50]}...")
                        continue
                    
                    file_path = DATA_DIR / filename
                    with open(file_path, 'w', encoding='utf-8') as f:
                        f.write(response_text)
                    log_with_time(f"Raw HTML saved to {file_path}")
                    
                    return response_text
                    
        except Exception as e:
            error_msg = f"Attempt {attempt + 1} failed with error: {str(e)}"
            log_with_time(error_msg)
            
        if attempt < max_retries - 1:
            delay = random.uniform(1, 3)
            log_with_time(f"Retrying in {delay:.1f} seconds...")
            await asyncio.sleep(delay)
        else:
            error_msg = f"All {max_retries} attempts failed for {url}"
            log_with_time(error_msg)
            await send_message_async(error_msg, is_error=True)
            return None

def save_html_content(response_text: str, filename: str) -> None:
    """ä¿å­˜HTMLå†…å®¹åˆ°dataç›®å½•ä¸‹çš„æ–‡ä»¶
    
    Args:
        response_text: è¦ä¿å­˜çš„HTMLå†…å®¹
        filename: æ–‡ä»¶å
    """
    try:
        file_path = DATA_DIR / filename
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(response_text)
        log_with_time(f"Raw HTML saved to {file_path}")
    except Exception as e:
        log_with_time(f"Error saving HTML content: {e}")

def get_last_articles_from_file(filename: str = LISTING_PARSED_FILE) -> set:
    """ä»æœ¬åœ°JSONæ–‡ä»¶ä¸­è¯»å–ä¸Šæ¬¡çš„æ–‡ç« IDé›†åˆ
    
    Args:
        filename: JSONæ–‡ä»¶åï¼Œé»˜è®¤ä¸º"listing_parsed.json"
        
    Returns:
        æ–‡ç« IDé›†åˆ
    """
    try:
        json_path = DATA_DIR / filename
        if not json_path.exists():
            log_with_time(f"No existing file found at {json_path}")
            return set()
            
        with open(json_path, 'r', encoding='utf-8') as f:
            json_data = json.load(f)
            
        # æŸ¥æ‰¾åŒ…å« catalogDetail çš„è·¯ç”±
        for route_content in json_data['appState']['loader']['dataByRouteId'].values():
            if 'catalogDetail' in route_content:
                articles = route_content['catalogDetail']['articles']
                log_with_time(f"Loaded {len(articles)} articles from {filename}")
                return {article['id'] for article in articles}
                
        log_with_time(f"No articles found in {filename}")
        return set()
    except Exception as e:
        log_with_time(f"Error reading last articles from file: {e}")
        return set()
