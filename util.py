import requests
import aiohttp
import random
from datetime import datetime
from typing import Optional, Dict, Any, List
import os
from pathlib import Path
import json

from config import WEBHOOK_URL, PROXY_URL, USE_PROXY, COOKIE

# User-Agentæ± 
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:120.0) Gecko/20100101 Firefox/120.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0"
]

# Emojiæ˜ å°„å­—å…¸
EMOJI_MAPPINGS = {
    # ä¸Šå¸å…¬å‘Š
    'listing': {
        "Introducing": "ğŸš€",
        "ä¸Šçº¿": "ğŸš€",
        "Launchpool": "ğŸŒ±", 
        "Futures": "ğŸ“ˆ",
        "Options": "ğŸ“Š",
        "Margin": "ğŸ’¹"
    },
    # æ–°é—»å…¬å‘Š
    'news': {
        "Binance": "ğŸ“¢",
        "å¸å®‰": "ğŸ“¢",
        "Announcement": "ğŸ“£",
        "å…¬å‘Š": "ğŸ“£",
        "Notice": "â„¹ï¸",
        "é€šçŸ¥": "â„¹ï¸"
    },
    # æ´»åŠ¨å…¬å‘Š
    'activities': {
        "Rewards": "ğŸ",
        "å¥–åŠ±": "ğŸ",
        "Campaign": "ğŸ¯",
        "æ´»åŠ¨": "ğŸ¯",
        "Airdrop": "ğŸª‚",
        "ç©ºæŠ•": "ğŸª‚",
        "Staking": "ğŸ†",
        "è´¨æŠ¼": "ğŸ†"
    }
}

# æ–‡ä»¶è·¯å¾„ç›¸å…³é…ç½®
DATA_DIR = Path("data")
DATA_DIR.mkdir(exist_ok=True)

# æ–‡ä»¶åé…ç½®
LISTING_RAW_FILE = "listing_raw.html"
LISTING_PARSED_FILE = "listing_parsed.json"

def get_emoji_for_type(title: str, announcement_type: str) -> str:
    """æ ¹æ®å…¬å‘Šæ ‡é¢˜å’Œç±»å‹è¿”å›ç›¸åº”çš„emoji
    
    Args:
        title: å…¬å‘Šæ ‡é¢˜
        announcement_type: å…¬å‘Šç±»å‹('listing', 'news', 'activities')
        
    Returns:
        å¯¹åº”çš„emojiå­—ç¬¦ä¸²
    """
    emoji_map = EMOJI_MAPPINGS.get(announcement_type, {})
    return next((emoji for keyword, emoji in emoji_map.items() 
                if keyword in title), "â„¹ï¸")

def build_article_link(article_id: str, category: str = '') -> str:
    """æ„å»ºæ–‡ç« é“¾æ¥
    
    Args:
        article_id: æ–‡ç« ID
        category: åˆ†ç±»('listing', 'news', 'activities')
        
    Returns:
        å®Œæ•´çš„æ–‡ç« URL
    """
    base_url = "https://www.binance.com/en/support/announcement/"
    
    # æ ¹æ®ä¸åŒç±»å‹æ·»åŠ ä¸åŒçš„å‚æ•°
    category_params = {
        'listing': '?c=48&navId=48',
        'news': '?c=49&navId=49',
        'activities': '?c=50&navId=50'
    }
    
    params = category_params.get(category, '')
    return f"{base_url}{article_id}{params}"

def build_message(title: str, 
                 release_date: str, 
                 link: str,
                 announcement_type: str,
                 is_initial: bool = False) -> str:
    """æ„å»ºé€šç”¨çš„æ¨é€æ¶ˆæ¯
    
    Args:
        title: å…¬å‘Šæ ‡é¢˜
        release_date: å‘å¸ƒæ—¶é—´
        link: æ–‡ç« é“¾æ¥
        announcement_type: å…¬å‘Šç±»å‹('listing', 'news', 'activities')
        is_initial: æ˜¯å¦ä¸ºåˆå§‹åŒ–æ¶ˆæ¯
        
    Returns:
        æ ¼å¼åŒ–çš„æ¶ˆæ¯å­—ç¬¦ä¸²
    """
    emoji = get_emoji_for_type(title, announcement_type)
    
    # æ ¹æ®ä¸åŒç±»å‹è®¾ç½®ä¸åŒçš„å‰ç¼€
    prefix_map = {
        'listing': "æ–°å¸ç§ä¸Šçº¿å…¬å‘Š",
        'news': "å¸å®‰æ–°é—»å…¬å‘Š",
        'activities': "å¸å®‰æ´»åŠ¨å…¬å‘Š"
    }
    
    if is_initial:
        prefix = f"ğŸ“¢ Initial {prefix_map.get(announcement_type, '')} Alert ğŸ“¢"
    else:
        prefix = f"{emoji} {prefix_map.get(announcement_type, '')} ğŸ“¢"
    
    return (
        f"{prefix}\n"
        f"æ ‡é¢˜: {title}\n"
        f"æ—¶é—´: {release_date}\n"
        f"é“¾æ¥: {link if link else 'æ— é“¾æ¥'}"
    )

async def send_message_async(message_content: str) -> None:
    """å‘é€æ¶ˆæ¯åˆ°ä¼ä¸šå¾®ä¿¡æœºå™¨äºº"""
    headers = {'Content-Type': 'application/json'}
    
    payload = {
        "msgtype": "text",
        "text": {
            "content": message_content
        }
    }
    
    proxy = PROXY_URL if USE_PROXY else None
    async with aiohttp.ClientSession() as session:
        async with session.post(WEBHOOK_URL, json=payload, headers=headers, proxy=proxy) as response:
            if response.status == 200:
                log_with_time("Message sent successfully!")
            else:
                log_with_time(f"Failed to send message: {response.status}")

def log_with_time(message: str, module: str = '') -> None:
    """æ‰“å°å¸¦æ—¶é—´æˆ³å’Œæ¨¡å—åçš„æ¶ˆæ¯
    
    Args:
        message: æ—¥å¿—æ¶ˆæ¯
        module: æ¨¡å—åç§°
    """
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    module_prefix = f"[{module}] " if module else ""
    print(f"[{current_time}] {module_prefix}{message}")

def get_random_headers(referer: str = '') -> Dict[str, str]:
    """ç”Ÿæˆéšæœºè¯·æ±‚å¤´"""
    return {
        'authority': 'www.binance.com',
        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
        'accept-encoding': 'gzip, deflate, br, zstd',
        'accept-language': 'zh-CN,zh;q=0.9,en;q=0.8,zh-TW;q=0.7',
        'cache-control': 'max-age=0',
        'User-Agent': random.choice(USER_AGENTS),
        'cookie': COOKIE,
        'sec-ch-ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
        'sec-ch-ua-mobile': '?0',
        'sec-ch-ua-platform': '"Windows"',
        'sec-fetch-dest': 'document',
        'sec-fetch-mode': 'navigate',
        'sec-fetch-site': 'same-origin',
        'sec-fetch-user': '?1',
        'upgrade-insecure-requests': '1'
    }

async def initialize_monitor(save_and_parse_func: callable, 
                           send_notifications_func: callable,
                           always_notify: bool = False) -> tuple:
    """é€šç”¨çš„ç›‘æ§åˆå§‹åŒ–å‡½æ•°"""
    initial_articles = save_and_parse_func()
    if initial_articles:
        article_ids = {article['id'] for article in initial_articles}
        log_with_time(f"Initialized with {len(article_ids)} articles")
        
        if always_notify:
            await send_notifications_func(initial_articles, is_initial=True)
            
        return article_ids, initial_articles
    return set(), []

async def check_new_articles(save_and_parse_func: callable, 
                           last_ids: set) -> tuple:
    """é€šç”¨çš„æ£€æŸ¥æ–°æ–‡ç« å‡½æ•°"""
    articles = await save_and_parse_func()
    if not articles:
        return None, set()
        
    current_ids = {article['id'] for article in articles}
    new_ids = current_ids - last_ids
    
    return articles, new_ids

async def fetch_and_save_html_content(url: str, filename: str) -> Optional[str]:
    """è·å–æŒ‡å®šURLçš„HTMLå†…å®¹å¹¶ä¿å­˜
    Args:
        url: è¦è¯·æ±‚çš„URL
        filename: ä¿å­˜HTMLå†…å®¹çš„æ–‡ä»¶å
    Returns:
        å“åº”çš„æ–‡æœ¬å†…å®¹ï¼Œå¦‚æœè¯·æ±‚å¤±è´¥åˆ™è¿”å›None
    """
    try:
        headers = get_random_headers(url)
        response = requests.get(url, headers=headers)
        log_with_time(f"Response status: {response.status_code}")
        
        if response.status_code != 200:
            log_with_time(f"Failed to fetch data: {response.status_code}")
            return None

        save_html_content(response.text, filename)
        return response.text
        
    except Exception as e:
        log_with_time(f"Error fetching data from {url}: {e}")
        return None

def save_html_content(response_text: str, filename: str) -> None:
    """ä¿å­˜HTMLå†…å®¹åˆ°dataç›®å½•ä¸‹çš„æ–‡ä»¶
    
    Args:
        response_text: è¦ä¿å­˜çš„HTMLå†…å®¹
        filename: æ–‡ä»¶å
    """
    try:
        file_path = DATA_DIR / filename
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(response_text)
        log_with_time(f"Raw HTML saved to {file_path}")
    except Exception as e:
        log_with_time(f"Error saving HTML content: {e}")

def get_last_articles_from_file(filename: str = LISTING_PARSED_FILE) -> set:
    """ä»æœ¬åœ°JSONæ–‡ä»¶ä¸­è¯»å–ä¸Šæ¬¡çš„æ–‡ç« IDé›†åˆ
    
    Args:
        filename: JSONæ–‡ä»¶åï¼Œé»˜è®¤ä¸º"listing_parsed.json"
        
    Returns:
        æ–‡ç« IDé›†åˆ
    """
    try:
        json_path = DATA_DIR / filename
        if not json_path.exists():
            log_with_time(f"No existing file found at {json_path}")
            return set()
            
        with open(json_path, 'r', encoding='utf-8') as f:
            json_data = json.load(f)
            
        # æŸ¥æ‰¾åŒ…å« catalogDetail çš„è·¯ç”±
        for route_content in json_data['appState']['loader']['dataByRouteId'].values():
            if 'catalogDetail' in route_content:
                articles = route_content['catalogDetail']['articles']
                log_with_time(f"Loaded {len(articles)} articles from {filename}")
                return {article['id'] for article in articles}
                
        log_with_time(f"No articles found in {filename}")
        return set()
    except Exception as e:
        log_with_time(f"Error reading last articles from file: {e}")
        return set()