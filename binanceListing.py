import asyncio
import json
import re
from typing import Optional, List, Dict, Any
from datetime import datetime

from config import COOKIE, ALWAYS_NOTIFY, LISTING_API_URL, MONITOR_INTERVAL
from util import (
    DATA_DIR,
    LISTING_RAW_FILE,
    LISTING_PARSED_FILE,
    send_message_async,
    log_with_time,
    build_article_link,
    build_message,
    check_new_articles,
    fetch_and_save_html_content,
)

last_article_ids = set()

def parse_listing_data(html_content: str) -> Optional[tuple[List[Dict[str, Any]], List[Dict[str, Any]]]]:
    """ä»HTMLå†…å®¹ä¸­è§£æå‡ºæ–°å¸ä¸Šçº¿ä¿¡æ¯,è¿”å›(articles, latest_articles)å…ƒç»„"""
    try:
        # æŸ¥æ‰¾åŒ…å«ç›®æ ‡æ•°æ®çš„scriptæ ‡ç­¾
        pattern = r'<script id="__APP_DATA" type="application/json".*?>(.*?)</script>'
        script_content = re.search(pattern, html_content, re.DOTALL)
        
        if not script_content:
            log_with_time("ğŸ”´ No APP_DATA script found")
            return None
            
        # è§£æJSONæ•°æ®
        json_data = json.loads(script_content.group(1))
        
        # ä¿å­˜è§£æåçš„JSONæ•°æ®åˆ°dataç›®ï¿½ï¿½
        json_path = DATA_DIR / LISTING_PARSED_FILE
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
        log_with_time(f"Parsed JSON saved to {json_path}")

        # æŸ¥æ‰¾åŒ…å« catalogDetail çš„è·¯ç”±
        route_data = None
        for route_content in json_data['appState']['loader']['dataByRouteId'].values():
            if 'catalogDetail' in route_content:
                route_data = route_content
                break
                
        if not route_data:
            log_with_time("ğŸ”´ No route with catalogDetail found") 
            return None
            
        # è·å–ä¸¤ä¸ªåˆ—è¡¨
        articles = route_data['catalogDetail']['articles']
        latest_articles = route_data.get('latestArticles', [])
        
        # ç»Ÿä¸€æ—¶é—´å­—æ®µ
        for article in latest_articles:
            article['releaseDate'] = article.pop('publishDate', 0)
            
        return articles, latest_articles
        
    except Exception as e:
        log_with_time(f"ğŸ”´ Error parsing listing data: {e}")
        return None

async def save_and_parse_listings() -> Optional[tuple[List[Dict[str, Any]], List[Dict[str, Any]]]]:
    """è·å–å¹¶è§£æä¸Šå¸å…¬å‘Šæ•°æ®"""
    log_with_time("Fetching and parsing listings...")
    response_text = await fetch_and_save_html_content(LISTING_API_URL, LISTING_RAW_FILE)
    if response_text is None:
        return None
    return parse_listing_data(response_text)

async def send_new_article_notifications(articles: List[Dict[str, Any]], 
                                       new_ids: set,
                                       is_initial: bool = False) -> None:
    """å‘é€æ–°æ–‡ç« é€šçŸ¥"""
    for article in articles:
        if article['id'] in new_ids:
            formatted_date = datetime.fromtimestamp(
                article['releaseDate']/1000
            ).strftime('%Y-%m-%d %H:%M:%S')
            
            # æ·»åŠ æ¥æºæ ‡è®°
            source = "Latest Articles" if 'catalogName' in article else "Listing"

            message = build_message(
                title=article['title'],
                release_date=formatted_date,
                link=build_article_link(article['id'], 'listing'),
                announcement_type=f"{source}",
                is_initial=is_initial
            )
            await send_message_async(message)

async def monitor() -> None:
    """ç›‘æ§æ–°å¸ä¸Šçº¿å…¬å‘Š"""
    global last_article_ids
    log_with_time("ğŸŸ¢ Starting Binance listing monitor...")
    
    while True:
        try:
            # æ£€æŸ¥æ–°æ–‡ç« 
            result = await save_and_parse_listings()
            if result is None:
                log_with_time("ğŸ”´ No articles found in this check")
                await asyncio.sleep(MONITOR_INTERVAL)
                continue
                
            articles, latest_articles = result
            # åˆå¹¶ä¸¤ä¸ªåˆ—è¡¨
            all_articles = articles + latest_articles
            
            # è·å–æ‰€æœ‰æ–‡ç« çš„IDé›†åˆ
            current_article_ids = {article['id'] for article in all_articles}
            
            # é¦–æ¬¡æ‰§è¡Œæ—¶,åªè®°å½•IDä¸æ¨é€
            if len(last_article_ids) == 0:
                log_with_time("ğŸ”´ First run, recording article IDs without notification")
                last_article_ids = current_article_ids
            
            # æ‰¾å‡ºæ–°æ–‡ç« ID
            new_article_ids = current_article_ids - last_article_ids
            
            if new_article_ids:
                log_with_time(f"ğŸŸ¢ Found {len(new_article_ids)} new articles")
                # print all new articles
                for article in all_articles:
                    if article['id'] in new_article_ids:
                        log_with_time(f"ğŸŸ¢ Article: {article['title']}")
                is_initial = len(last_article_ids) == 0 and ALWAYS_NOTIFY
                await send_new_article_notifications(all_articles, new_article_ids, is_initial)
            else:
                log_with_time("No new articles found")
            
            # æ›´æ–°ä¸Šæ¬¡çš„æ–‡ç« IDé›†åˆ
            last_article_ids = current_article_ids
            
        except Exception as e:
            log_with_time(f"ğŸ”´ Error in monitor loop: {e}")
            await send_message_async(f"âŒ Monitor Error: {str(e)}")
        
        await asyncio.sleep(MONITOR_INTERVAL)

if __name__ == "__main__":
    asyncio.run(monitor())
